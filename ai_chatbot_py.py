# -*- coding: utf-8 -*-
"""AI chatbot.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HDBUF-kJtQhAQzzBYaFF1WsVkn5Z3jPC
"""

import streamlit as st
import tempfile
import os
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from transformers import pipeline
from langchain_community.llms import HuggingFacePipeline
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# Streamlit UI setup
st.set_page_config(page_title="AI Compliance Chatbot", layout="centered")
st.title(" AI Policy Compliance Chatbot")
st.caption("Upload AI regulation PDFs and ask compliance-related questions.")

# Hugging Face Token (Optional: Replace with your token or use local models)
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "your_token_here"

# Upload PDF
uploaded_file = st.file_uploader("ðŸ“Ž Upload an AI policy PDF", type="pdf", accept_multiple_files=True)

if uploaded_file:
    with st.spinner("Processing document..."):
        # Save uploaded file
        docs = []
        for uploaded_file in uploaded_file:
           with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
               tmp_file.write(uploaded_file.read())
               file_path = tmp_file.name
        loader = PyPDFLoader(file_path)
        docs.extend(loader.load())

        # Load and split PDF
        loader = PyPDFLoader(file_path)
        docs = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
        split_docs = text_splitter.split_documents(docs)

        # Embeddings and vector store
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        vectorstore = FAISS.from_documents(split_docs, embeddings)
        retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

        # LLM setup
        llm_pipeline = pipeline("text2text-generation", model="google/flan-t5-large", max_new_tokens=300)
        llm = HuggingFacePipeline(pipeline=llm_pipeline)

        # Conversation memory and QA chain
        memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )

        qa_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=retriever,
            memory=memory,
            return_source_documents=True,
            output_key="answer"
        )

        # Chat interaction
        if "chat_history" not in st.session_state:
            st.session_state.chat_history = []

        user_input = st.text_input("Ask a compliance question:")
        if user_input:
            with st.spinner("Generating response..."):
                result = qa_chain.invoke({"question": user_input})
                st.session_state.chat_history.append(("You", user_input))
                st.session_state.chat_history.append(("Bot", result["answer"]))

                # Display response
                st.markdown(f"**Bot:** {result['answer']}")

                # Show source context
                with st.expander(" Source"):
                   st.write(result["source_documents"][0].page_content[:1000])


        # Display full chat history
        if st.session_state.chat_history:
            st.markdown("---")
            st.markdown("### Chat History")
            for speaker, msg in st.session_state.chat_history:
                st.markdown(f"**{speaker}:** {msg}")